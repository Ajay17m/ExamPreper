# LangChain Agent Quiz (Gemini 2.0 Flash)

A hands-on project to learn the **LangChain Agent** framework by building a full quiz app that:
- Reads an attached JSON question bank (includes **SingleOption**, **TwoOption**, **Match** types).
- Picks **50** random questions with no repetition.
- Generates **15** additional questions **on the fly** using **Gemini 2.0 Flash** via LangChain.
- Enforces a **90-minute** time limit.
- Tracks score and shows **correct / total** and **percentage** at the end.
- Produces a **brief study summary** for every incorrectly answered question, generated by Gemini 2.0 Flash.

Tech stack:
- UI: **Streamlit**
- LLM: **Gemini 2.0 Flash** through `langchain_google_genai`
- Agent: **LangChain tool-calling agent** with two tools
- Hosting: **Hugging Face Spaces** (free) + **GitHub Pages** (as an embedded iframe)

---

## 1) Run locally

### Prereqs
- Python 3.10+
- A Google API key with access to **Gemini 2.0 Flash**

### Setup
```bash
git clone https://github.com/<your-username>/langchain-quiz-app.git
cd langchain-quiz-app

# Create and activate a venv (optional but recommended)
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install deps
pip install -r requirements.txt

# Configure env
cp .env.example .env
# Edit .env and set GOOGLE_API_KEY=...
```

### Start the app
```bash
streamlit run app.py
```

The app will load `QnA2.json`, pick 50 random questions, and then generate the remaining 15 during the exam at milestones (10, 25, 40).

> If you want to pre-generate all 15 at the beginning, see `add_questions_from_llm_if_needed()` in `app.py` and adjust logic accordingly.

---

## 2) Understand the LangChain Agent

The agent lives in **`agent_utils.py`** and exposes two tools:

- `generate_questions(n, topics_hint, allowed_types)` → returns new questions in the **same JSON schema**.
- `summarize_incorrect_topics(questions_with_answers)` → returns a concise markdown study guide for missed questions.

We create a tool-calling agent with:

```python
from agent_utils import create_agent
agent_exec, tools = create_agent()
```

And we also call the tools directly from the UI for simplicity:

```python
new_json = generate_questions.invoke({"n": 5, "topics_hint": "RAG, evaluation, embeddings"})
```

> Both approaches (agent invocation vs. direct tool call) use the **same LLM (Gemini 2.0 Flash)** from `langchain_google_genai`.

---

## 3) JSON schema (read and supported)

The app expects the uploaded dataset in `QnA2.json` to contain these top-level arrays:

- `SingleOption`: radio with one correct choice
- `TwoOption`: checkboxes with **exactly two** correct choices
- `Match`: left-hand **Choices** (A, B, C, …) matched to right-hand **options** (A, B, C, …)

Examples are included in the provided `QnA2.json`. The **Match** format maps each `Choices` key to a letter in `options` in the same order (A → Answer.option[0], B → Answer.option[1], etc.).

---

## 4) Deploy to **Hugging Face Spaces** (free)

1. Create a new Space: **New → Space → Streamlit**.
2. Upload all files from this repo to the Space (or connect your GitHub repo).
3. In the Space **Settings → Secrets**, add:
   - `GOOGLE_API_KEY` → your key
4. The Space will automatically install from `requirements.txt` and run `app.py`.

> Tip: Add a custom thumbnail and README in your Space for polish.

---

## 5) Mirror to **GitHub Pages** (free)

GitHub Pages is static, so you can't run Python there. Two easy options:

### Option A — Embed your Hugging Face Space

1. Create a **`docs/`** folder in your GitHub repo and place the provided `index.html` (iframe) inside.
2. In repo **Settings → Pages**, set **Source** to **Deploy from a branch** and **Branch** to **main /docs**.
3. Replace `YOUR_SPACE_PATH` in `index.html` with your HF Space path (e.g., `username/space-name`).

### Option B — Docs only

Use Pages to host **documentation** and link to your working HF Space. This is the simplest setup.

---

## 6) Customize / Extend

- **Question difficulty buckets**: track and adaptively serve harder/easier questions.
- **Per-topic scoring**: tag each question and add topic-wise breakdown.
- **Persistence**: store results to a DB (e.g., Supabase) with user auth.
- **RAG for question generation**: plug `GoogleGenerativeAIEmbeddings` + a vector store to bias LLM-generated questions to your notes.

---

## 7) Troubleshooting

- **Model access**: Ensure your API key has access to **`gemini-2.0-flash`**.
- **Spaces build**: If the Space fails, reopen the logs. Often missing `GOOGLE_API_KEY` is the cause.
- **Parsing errors**: If the generator returns malformed JSON, try again or tweak temperature (see `agent_utils.get_llm()`).
- **Timer**: The timer is enforced server-side—answers after 90 minutes won't be accepted.

---

## License

MIT
